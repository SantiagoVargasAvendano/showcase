[{"id":0,"href":"/showcase/docs/shortcodes/integrantes/","title":"Integrantes","section":"Shortcodes","content":" Integrantes # Emmanuel Steven Rojas Arcila # Perfil # Estudiante de Sistemas y Computación cursando noveno semestre en la Universidad nacional de Colombia. Con intereses en las ramas de la criptografía y desarrollo de software con enfoques en el sector del frontend. Entre sus intereses fuera del área académica se encuentran realizar ejercicio y ver películas. Santiago Vargas Avendaño # Perfil # Es un estudiante de ingeniería de sistemas y computación en noveno semestre, entre sus principales intereses académicos están la rama del desarrollo e ingeniería de software con especial enfoque en el backend. Los principales pasatiempos son el Voleibol, las manualidades (origami, tejido, bordado) y las series de televisión. "},{"id":1,"href":"/showcase/docs/shortcodes/referencias/","title":"Referencias","section":"Shortcodes","content":" Referencias # Primer taller # Ilusión de la pirámide - Michael Bach Ilusión dinámica de Ebbinghaus - Michael Bach Efecto estereoquinético - Michael Bach Inhibición lateral Profundidad estereoquinética Ebbinghaus Segundo taller # Aliasing \u0026amp; Anti-aliasing Computer Graphics Antialiasing Aliasing Rasterization: a Practical Implementation 3D user interaction World transform Tercer taller # Normal mapping base idea Normal mapping Region of interest Image processing Color inversion Luma "},{"id":2,"href":"/showcase/docs/shortcodes/registros/","title":"Registros","section":"Shortcodes","content":" Registros # En está página se mantienen el registro de los encuentros\nPrimer taller # 29/marzo/2022 Primera entrega de propuesta hacia el profesor de 3 ilusiones visuales:\nIlusión de la piramide Ilusión dinámica de Ebbinghaus Efecto estereoquinético también se planteó la idea de entregar un programa de \u0026ldquo;Todo es Om\u0026rdquo; pero por motivos de alcance de la entrega se concluyó que no era ideal. 31/marzo/2022 En esta sesión se presentó al profesor la ilusión dinámica de Ebbinghaus y la ilusión de la piramide, a lo cual se recibieron sus respectivas recomendaciones y sugerencias por parte del profesor, durante la clase se trabajó para poder determinar bien las coordenadas en las ilusiones. Segundo taller # 03/mayo/2022 Esta fue la primera sesión para la realización del taller, en esta revisamos cuales eran los posibles ejercicios a realizar para esta segunda entrega y que herramientas teniamos que utilizar para llevarlos a cabo. 05/mayo/2022 Esta sesión fue la primera aproximación que tuvimos con la libreria de Quadrille, revisamos toda la documentación necesaria y se habló con el profesor sobre el ejercicio que se iba a realizar, de esta charla se encontró que a partir del metodo de RasterizeTriangle ibamos a poder hacer la implementación de Anti-aliasing. 10/mayo/2022 En esta sesión se hizo la instalación y el uso del mousePad que el profesor llevó a la clase para poder entender como funcionaba este y poder averiguar si podria ser o no implementado con el proyecto, dado el tiempo que se requería y el no contar con el de manera permanente se decidió hacer uso de la tableta gráfica. 12/mayo/2022 Durante esta sesión fue el avance teórico sobre como se va a realizar el Anti-aliasing y fue brindado por el profesor una guía sobre como podiamos hacer la implementación. 17/mayo/2022 En esta sesión dado que no se realizó sesión sincrónica de la clase en la universidad no se presentó ningun avance con el prodesor, sin embargo, el espacio de la clase fue aprovechado para hacer la primera aproximación de la tableta gráfica con el entorno de p5. 19/mayo/2022 Sesión final antes de la presentación durante la cual se mostró al profesor lo realizado por el Anti-aliasing y el sugirió añadir un sombreado al mismo, lo cual se implementó en la misma clase. Tercer taller # 14/junio/2022 En esta primera sesión se llegó con los adelantos de lo que es el trabajo presentado en texturing, con lo cual se recibió retroalimentación del profesor sobre la manera que podíamos utilizar para teñir y otros métodos a implemetar (teñido con interpolado) 16/junio/2022 En esta sesión se hizo muestra del texturing finalizado con las recomendaciones del profesor y un avance en lo que respecta al código de masking, le comentamos nuestros problemas con el tema del zoom a lo cual el nos guió sobre como podiamos orientar nuestra busqueda para la solución. Ademas el profesor nos sugirió explorar el tema de bump mapping. 23/junio/2022 En esta última sesión se habló sobre lo del tema del zoom y como habiamos resulto una parte del problema. También siguiendo la recomendación del profesor en la sesión anterior se le mostró el código que se hizo pero no sobre bump sino sobre normal mapping que es una aplicación de bump mapping. "},{"id":3,"href":"/showcase/docs/shortcodes/Taller-1/","title":"Taller 1","section":"Shortcodes","content":" Taller 1 # Esto fue el trabajo realizado para la primera entrega de computación visual.\n"},{"id":4,"href":"/showcase/docs/shortcodes/Taller-1/ebbinghaus/","title":"Ebbinghaus","section":"Taller 1","content":" Ilusión dinámica de Ebbinghaus # Planteamiento del problema # Dentro del sitio web de Michael Bach se encontró una ilusión óptica que es la versión dinámica de \u0026ldquo;La ilusión de Ebbinghaus\u0026rdquo;. La ilusión consta de un anillo de discos azules y en su centro un disco naranja y todo se mueve, los discos azules cambian de tamaño pero el disco naranja permanece siempre con un tamaño constante, sin embargo, cuando los discos azules alcanzan el mayor tamaño el disco naranja parece haber disminuido de tamaño y de igual manera cuando los discos azules alcanzan su menor tamaño el disco naranja parece haber aumentado su tamaño.\nAntecedentes # Esta ilusión corresponde a una variación dinámica de la ilusión de Ebbinghaus, variación creada por Christopher D. Blair, Gideon P. Caplovitz y Ryan E.B. Mruczek, que logró potenciar la fuerza de la ilusión y fue ganadora en el \u0026ldquo;Visual Illusion Contest\u0026rdquo; de 2014. En la ilusión de Ebbinghaus, se ven dos anillos de discos azules uno con los discos grandes y otro con los discos pequeños, en el centro de ambos anillos hay dos discos naranjas que a pesar de que no lo parezcan tienen igual tamaño; la ilusión lleva el nombre de Hermann Ebbinghaus, un pionero en la investigación de la memoria, que probablemente descubrió esta ilusión en la década de 1890, pero no la divulgó en ninguna publicación específica. Más adelante, Titchener (sin reclamar su autoría) la publicó en un libro de texto de 1901; por ello, también se le suele conocer como la ilusión de Titchener.\nCódigo (solución) y resultados # Código en p5.js let x = 400,y = 400, L = 22, A, H, rad, distance, aux = 1; function setup() { createCanvas(500, 500); frameRate(40); A = cos(PI/3)*L; H = sin(PI/3)*L; } function draw() { background(191); if((frameCount-1) % 125 == 0){ aux = -aux; } x = x + aux * 1.25; y = y + aux * 1.25; noStroke(); fill(235, 131, 26); ellipse(x,y,30); fill(80, 125, 230); rad = 12*pow((400/x),5); distance = pow((400/x),4) ellipse(x-L*distance,y,rad); ellipse(x+L*distance,y,rad); ellipse(x+A*distance,y-H*distance,rad); ellipse(x-A*distance,y-H*distance,rad); ellipse(x+A*distance,y+H*distance,rad); ellipse(x-A*distance,y+H*distance,rad); strokeWeight(5); stroke(230, 216, 32); point(x,y); stroke(0, 60, 138); point(x-A*distance,y-H*distance); } Conclusiones y trabajo futuro # Basados en los resultados y en un proceso de observación, se concluye que existe una tendencia a aumentar el fenómeno visual generado cuando la visión del observador se fija en la circunferencia interior plasmada dentro del disco azul superior izquierdo. La ilusión de Ebbinghaus ha desempeñado un papel crucial en el debate sobre la existencia de vías separadas en el cerebro para la visión de percepción y visión de la acción. Se ha argumentado que la ilusión de Ebbinghaus distorsiona la percepción del tamaño, pero no para la acción. Así mismo se encontró un estudio en el cual muestran que existen 70 variantes genéticas relacionadas con la percepción de la ilusión de Ebbinghaus. Uno de los distintos campos en los que se puede seguir trabajando con la ilusión de Ebbinghaus es con la percepción de colores y si estos afectan de alguna manera la percepción del tamaño, también se puede considerar analizar si la forma en la que se está moviendo el sistema afecta que tanto se percibe la ilusión.\n"},{"id":5,"href":"/showcase/docs/shortcodes/Taller-1/estereoquinetico/","title":"Estereoquinetico","section":"Taller 1","content":" Efecto estereoquinético # Planteamiento del problema # En la profundidad estereo cinética, el movimiento de rotación aumenta la impresión de profundidad que se observa en los patrones de anillos concéntricos. Normalmente, la rotación crea una impresión vívida de un cono que sobresale o un túnel que retrocede, o ambos.\nAntecedentes # La primera vez que el término de profundidad estereoquinética fue descrito, fue en 1924 por Musatti CL en un informe científico (Sui fenomeni stereocineti) sobre la ilusión estereoquinética, atribuyendo el descubrimiento y el nombre a su maestro Vittorio Benussi. Este también fue usado cerca del año 1935 por el artista Marcel Duchamp, que llamaba \u0026ldquo;rotorrelieves\u0026rdquo; a sus discos giratorios.\nCódigo (solución) y resultados # Código en p5.js let movement = 0, stop = 1, crater = 1; function setup() { createCanvas(500, 500); background(255); frameRate(30); noStroke(); buttonStop = createButton(\u0026#39;Parar\u0026#39;); buttonStop.position(0, 478); buttonStop.mousePressed(stop_move); buttonReset = createButton(\u0026#39;Resetear\u0026#39;); buttonReset.position(430, 0); buttonReset.mousePressed(reset); buttonInvert = createButton(\u0026#39;Crater\u0026#39;); buttonInvert.position(447, 478); buttonInvert.mousePressed(invert); buttonInvert.style(\u0026#39;background-color\u0026#39;, color(255)); } function invert() { crater *= -1; if(crater \u0026gt; 0){ buttonInvert.style(\u0026#39;background-color\u0026#39;, color(255)); }else{ buttonInvert.style(\u0026#39;background-color\u0026#39;, color(125)); } } function reset() { movement = 0; stop = 1; crater = 1; buttonInvert.style(\u0026#39;background-color\u0026#39;, color(255)); } function stop_move() { stop *= -1; } function draw() { let aux = 2; fill(0,0,255); ellipse(250,250,500); for (let i = 1; i \u0026lt; 12; i++) { if(i % 2 == 0){ fill(0,0,255); }else{ fill(255,255,0); } if(crater==1){ ellipse(250+(cos((movement % 360)*PI/180)*i*20),250+(sin((movement % 360)*PI/180)*i*20),500-i*40); }else{ if(i \u0026lt; 7){ ellipse(250+(cos((movement % 360)*PI/180)*i*20),250+(sin((movement % 360)*PI/180)*i*20),500-i*40); aux = i - 1; }else{ ellipse(250+(cos((movement % 360)*PI/180)*aux*20),250+(sin((movement % 360)*PI/180)*aux*20),500-i*40); aux -= 1; } } } if(stop == 1){ movement += 1; } } Manejo de la ilusión # En la parte superior derecha se encuentra un botón de \u0026ldquo;Resetear\u0026rdquo; el cual devuelve la ilusión a su estado original que es sin cráter y la posición inicial; en la esquina inferior izquierda se encuentra el botón de \u0026ldquo;Parar\u0026rdquo; con el cual la ilusión interrumpe su rotación hasta que este sea pulsado de nuevo; por último en la esquina inferior derecha se encuentra un boton de \u0026ldquo;cráter\u0026rdquo; el cual sirve para generar dentro de la ilusión un efecto de profundidad.\nConclusiones y trabajo futuro # Tomando en cuenta los resultados de la codificación y las pruebas visuales del mismo, se puede evidenciar cómo el movimiento de rotación en los círculos genera la impresión de profundidad. Como implementaciones a desarrollar para próximos intentos, se sugiere la idea de estudiar la combinación otro tipo de figuras y cómo eso cambia la dinámica de rotación, adicionalmente se sugiere que se tengan en cuenta los componente principales de la ilusión, tanto la rotación como la rapidez con la que se realiza, y sus relaciones o capacidades de potenciar las ilusiones propuestas.\n"},{"id":6,"href":"/showcase/docs/shortcodes/Taller-1/piramide/","title":"Piramide","section":"Taller 1","content":" Ilusión de la pirámide # Planteamiento del problema # Se encontró dentro del archivo web del investigador Michael Bach sobre fenómenos visuales, una ilusión óptica denominada “ilusión de la pirámide” efecto perceptivo relacionado con todos los fenómenos que implican inhibición lateral.\nLa inhibición lateral se define como el proceso por el cual una célula inhibe la actividad de las células adyacentes. En las células de la retina, la inhibición lateral genera un realce de los bordes y un mayor contraste en las imágenes que se forman en el cerebro. El efecto de ésta inhibición lateral fue descubierto por Ernst Mach, que explicó en 1865 la ilusión visual denominada bandas de March. Este efecto produce que paneles que generan diferentes sombras colocados uno al lado del otro aparezcan más claros o más oscuros en las transiciones, a pesar del color uniforme dentro de un panel. Los paneles aparecen más claros en el borde con un panel más oscuro, y más oscuros en el borde con un panel más claro.\nEl propósito de la ilusión yace en generar la sensación de la existencia de una “X” dentro de cuadrados concéntricos en sus esquinas cuando realmente no existe. Para esto, se implementó un código que generase cuadrados concéntricos uno encima de otro, en el cual se va degradando ligeramente el color del cuadro siguiente, con lo cual, conforme mayor sea la cantidad de cuadros generados, la ilusión óptica empieza a hacer su aparición.\nAntecedentes # Se tiene conocimiento de que fue demostrado por primera vez por Martinez-Conde \u0026amp; Macknik en la Reunión Soc Neurosci de 2001. Se ha incorporado a muchas pinturas Op Art como Arcturus II de Victor Vasarely.\nCódigo (solución) y resultados # Código en p5.js let maxSixe = 500, h, maxMumberSquares = 50, squares = 1, color_val = \u0026#34;Azul\u0026#34;, invert_flag = 1; function setup() { createCanvas(maxSixe+100, maxSixe); background(255); noStroke(); frameRate(20); buttonUp = createButton(\u0026#39;⯅\u0026#39;); buttonUp.position(560, 200); buttonUp.mousePressed(changeUp); buttonDown = createButton(\u0026#39;⯆\u0026#39;); buttonDown.position(560, 220); buttonDown.mousePressed(changeDown); buttonReset = createButton(\u0026#39;Resetear\u0026#39;); buttonReset.position(525, 100); buttonReset.mousePressed(reset); buttonInvert = createButton(\u0026#39;Invertir\u0026#39;); buttonInvert.position(530, 400); buttonInvert.mousePressed(invert); buttonInvert.style(\u0026#39;background-color\u0026#39;, color(255)); color_sel = createSelect(); color_sel.position(530, 300); color_sel.option(\u0026#39;Azul\u0026#39;); color_sel.option(\u0026#39;Verde\u0026#39;); color_sel.option(\u0026#39;Rojo\u0026#39;); color_sel.selected(\u0026#39;Azul\u0026#39;); color_sel.changed(mySelectEvent); } function invert() { invert_flag *= -1; if(invert_flag \u0026gt; 0){ buttonInvert.style(\u0026#39;background-color\u0026#39;, color(255)); }else{ buttonInvert.style(\u0026#39;background-color\u0026#39;, color(125)); } } function reset() { squares = 1; color_val = \u0026#34;Azul\u0026#34;; color_sel.selected(\u0026#39;Azul\u0026#39;); } function mySelectEvent() { color_val = color_sel.value(); } function changeUp() { if(squares+1 \u0026lt; maxMumberSquares){ squares += 1; } } function changeDown() { if(squares-1\u0026gt;0){ squares -= 1; } } function draw() { fill(255); rect(530,200,20,45) textSize(15); fill(0); text(squares, 530,210,20,60); if(squares \u0026lt; maxMumberSquares){ h = maxSixe/(2*squares); for(let i = 0; i \u0026lt; squares; i++){ if(invert_flag \u0026gt; 0){ if(color_val == \u0026#34;Rojo\u0026#34;){ fill(255,0+i*(255/squares),0+i*(255/squares)); }else if(color_val == \u0026#34;Verde\u0026#34;){ fill(0+i*(255/squares),255,0+i*(255/squares)); }else{ fill(0+i*(255/squares),0+i*(255/squares),255); } }else{ if(color_val == \u0026#34;Rojo\u0026#34;){ fill(255-i*(255/squares),0,0); }else if(color_val == \u0026#34;Verde\u0026#34;){ fill(0,255-i*(255/squares),0); }else{ fill(0,0,255-i*(255/squares)); } } rect(0+i*h,0+i*h, 500-i*2*h); } } } Manejo de la ilusión # En la parte derecha de la ilusión se encuentran los controles para manejarla, el primero es un botón de \u0026ldquo;Resetearla\u0026rdquo; que lleva la ilusión a sus valores por defecto que es 1 cuadro y el color azul; seguido encontramos dos botones junto con un número a su lado el botón con la flecha hacia arriba aumenta la cantidad de cuadrados (hasta 50), el botón con la flecha hacia abajo disminuye la cantidad de cuadrados (hasta 1) y el número al lado de ellos muestra la cantidad actual de cuadrados; luego hayamos un menú desplegable en el cual podemos escoger cualquiera de los tres colores con los cuales se presenta la ilusión de la pirámide y por ultimo se encuentra un botón para “Invertir” la ilusión, es decir, que el degrade desde el color principal sea hacia blanco o hacia negro.\nConclusiones y trabajo futuro # La ilusión de la pirámide es un llamativo efecto perceptivo relacionado con todos los fenómenos que implican inhibición lateral. Como se puedo observar, al generar varios cuadrados y conforme esta cantidad aumentaba el efecto visual de la aparición de una “X” en medio del escenario, adicionalmente, dicho efecto y su impresión en el observador podría cambiar dependiendo del color al que se dezplacen los cuadrados, blanco o negro, generando una sensación de realce o profundidad respectivamente. La explicación consiste en considerar las disposiciones de los campos receptivos de las células ganglionares antagónicas del centro-rededor que conviven con la imagen. Imaginemos una célula en una esquina con su centro en la mancha más clara, será inhibida por 1/4 de entorno de las manchas más claras y por 3/4 de las manchas más oscuras, así las células ganglionares de los bordes señalan más luminosidad. La percepción se integra con la información de las células ganglionares y se elimina la fuerte distorsión de la luminancia.\n"},{"id":7,"href":"/showcase/docs/shortcodes/Taller-2/","title":"Taller 2","section":"Shortcodes","content":" Taller 2 # Esto fue el trabajo realizado para la segunda entrega de computación visual.\n"},{"id":8,"href":"/showcase/docs/shortcodes/Taller-2/antialiasing/","title":"Antialiasing","section":"Taller 2","content":" Anti-Aliasing # Planteamiento del problema # Considere un triángulo renderizado, si nos acercamos lo suficiente sobre la imagen del mismo, notaremos que los bordes del triángulo no son regulares, en cambio tiene un patrón de “escalera”, de “sierra” o de “tablero de ajedrez”. Dichos bordes que son observados, son llamados “jaggies” o bordes escalonados, y son el resultado de lo que en realidad está sucediendo, que no es más que la figura, en este caso el triángulo, se encuentra dividida en píxeles. En el proceso de rasterización lo que se realiza es descomponer una superficie continua, el triángulo, en elementos discretos, los píxeles. A este efecto en computación gráfica, se le conoce con el nombre de aliasing, que de forma rápida se puede ver como “el escalonamiento visual de los bordes que se produce en una imagen cuando la resolución es demasiado baja”. Siendo más precisos en su definición, el aliasing es el defecto gráfico característico que hace que en una pantalla ciertas curvas y líneas inclinadas presenten un efecto visual tipo “sierra” o “escalón”, dicho defecto se presenta cuando se intenta representar una imagen con curvas y líneas inclinadas en una pantalla, framebuffer o imagen, pero que debido a la resolución finita de la misma resulta que este sea incapaz de representar la curva como es, y por tanto dichas curvas se muestran en pantalla dentadas al estar compuestas por los píxeles.\nAntecedentes # En consecuencia, para abordar los problemas generados por el aliasing, en computación visual se han desarrollado a lo largo de varias décadas algoritmos que buscan disminuir el impacto que tiene este defecto en el aspecto visual de los gráficos, entre los que resaltan nombres como el de Herbert Freeman en 1974 con \u0026ldquo;Computer processing of line drawing images\u0026rdquo;, o Edwin Catmull con trabajos como “A Subdivision Algorithm for Computer Display of Curved Surfaces” o \u0026ldquo;A hidden-surface algorithm with anti-aliasing\u0026rdquo; en los años 1974 y 1978 respectivamente. Dichos algoritmos se han determinado como algoritmos de anti-aliasing, o también conocido simplemente como AA, cuya definición más básica sería la de buscar “el suavizado de los bordes irregulares en las imágenes digitales promediando los colores de los píxeles en las fronteras”.\nFigura [1]. La letra de la izquierda sufre de aliasing. A la letra de la derecha se le ha aplicado anti-aliasing para que los bordes parezcan más suaves.\nEn cambio de renderizar con una única muestra por píxel, se divide el píxel en sub-píxeles, lo que conforme a mayor cantidad de los mismos, permite ilustrar los bordes de los objetos con mayor precisión. En la actualidad existen distintos métodos para tratar el problema del aliasing, Using high-resolution display, Post filtering (Supersampling), Pre-filtering (Area Sampling), Pixel phasing.\nHacer uso de dispositivos con mayor resolución - Una de las soluciones que se presentan para reducir el efecto visual causado por el aliasing es utilizar pantallas o dispositivos que sean de alta definición, con esto los “jaggies” o “bordes dentados” serán conforme mayor resolución cada vez más indistinguibles para el ojo humano, con lo cual estos bordes se tornan difusos y comenzarán a parecer más lisos.\nPre-filtración, Muestreo de área - En el muestreo de área, la intensidad de los píxeles es determinada calculando las áreas superpuestas de cada píxel con los objetos que van a ser desplegados, considerando los píxeles como áreas. Este método es conocido como pre-filtración ya que es un procedimiento que se hace antes de generar la imagen rasterizada.\nFases de pixel - El método se basa en desplazar la posición de los píxeles a posiciones aproximadas a la geometría del objeto. Algunos sistemas permiten que se ajuste el tamaño de los píxeles individuales para distribuir las intensidades, lo que es útil en la fase de píxeles.\nPost-filtración, Sobremuestreo - Los objetos son muestreados con una resolución superior y son desplegados con una resolución menor. En este método se incrementa la resolución de muestreo suponiendo que la pantalla estuviera compuesta de una cuadrícula más delgada de lo que realmente es. Para el cálculo de la intensidad de los píxeles se realiza al combinar o promediar los valores de las intensidades de los subpixeles que lo componen. Este método es conocido como post-filtración debido a que el procedimiento es realizado después de generar la imagen rasterizada. Un estilo mejorado de AA es el MSAA (antialiasing multimuestreo), que es un método más rápido y aproximado de supermuestreo AA, que tiene un menor costo computacional. Actualmente, las grandes empresas de tarjetas gráficas como CSAA de NVIDIA y CFAA de AMD desarrollan técnicas de sobremuestreo mejores y más sofisticadas, con el objetivo de resaltar en el mercado y mejorar el rendimiento de sus hardwares.\nCódigo (solución) y resultados # Código en p5.js function AA(quadrilleF, points, row0, col0, row1, col1, row2, col2, shader, pattern0, pattern1 = pattern0, pattern2 = pattern0) { if (Array.isArray(pattern0) \u0026amp;\u0026amp; Array.isArray(pattern1) \u0026amp;\u0026amp; Array.isArray(pattern2)) { for (let i = 0; i \u0026lt; this.height; i++) { for (let j = 0; j \u0026lt; this.width; j++) { let numberPoints = 0; for (let k = 0; k \u0026lt; points; k++) { for (let h = 0; h \u0026lt; points; h++) { let coords = quadrilleF._barycentric_coords(i + 0.01 + k*(0.98)/(points-1), j + 0.01 + h*(0.98)/(points-1), row0, col0, row1, col1, row2, col2); if (coords.w0 \u0026gt;= 0 \u0026amp;\u0026amp; coords.w1 \u0026gt;= 0 \u0026amp;\u0026amp; coords.w2 \u0026gt;= 0) { numberPoints++; } } } if (numberPoints \u0026gt; 0) { let coords = this._barycentric_coords(i, j, row0, col0, row1, col1, row2, col2); let length = Math.max(pattern0.length, pattern1.length, pattern2.length); let _pattern = new Array(length); for (let k = 0; k \u0026lt; _pattern.length; k++) { _pattern[k] = (pattern0[k] ?? 0) * coords.w0 + (pattern1[k] ?? 0) * coords.w1 + (pattern2[k] ?? 0) * coords.w2; } for (let k = 0; k \u0026lt; _pattern.length; k++) { _pattern[k] = 255 - (255 - _pattern[k])/(points*points)*numberPoints; } quadrilleF._memory2D[i][j] = shader({ pattern: _pattern, row: i, col: j }); } } } } } Para generar un triángulo con Anti-aliasing presione la tecla \u0026ldquo;r\u0026rdquo;\nPara la realización de este ejercicio de Anti-aliasing nos basamos en el método de Supersample Anti-aliasing (SSAA), mediante el cual por cada pixel existente dentro de la grilla original generamos una subgrilla virtual (la cantidad de filas y columnas de esta puede ser modificada por el usuario) dentro de cada uno de estos y mediante el uso de las coordenadas baricéntricas determinamos cual es la cantidad de subpíxeles dentro de cada pixel que se encuentra a su vez dentro de las coordenadas del triángulo, valga la pena recalcar que entre mayor sea el tamaño de la subgrilla virtual mayor cantidad de subpíxeles van a existir y el muestreo será más exacto y por lo tanto se tendrán resultados visuales más refinados y sutiles para el ojo humano.\nA partir de la cantidad de subpixeles que se encuentren dentro del triangulo podemos tomar la decisión sobre cual es la influencia que tiene este pixel dentro de nuestro muestreo de color, de esa manera asignarle el tono correspondiente, siendo los pixeles que menos puntos tienen dentro del triángulo los que poseen un color más tenue o cercano al blanco. De la misma manera se implementó gracias al uso de las coordenadas baricéntricas un sombreado del triángulo, donde cada una de sus vértices tiene un color (que puede ser escogido por el usuario en la animación) y a partir de estas se calcula el color de cada uno de los puntos coincidentes en la construcción del triángulo.\nConclusiones y trabajo futuro # Es importante resaltar la relevancia que tienen las coordenadas baricéntricas y sus potenciales usos en el área de computación gráfica. Como pudimos observar durante la experimentación y la investigación acerca del tema, las coordenadas fueron una herramienta vital para la construcción de nuestro ejemplo, permitiendo submuestrear cada pixel basados en la idea identificar gracias a los valores arrojados por las coordenadas si el subpixel estaba en el interior o no del triángulo. Durante las búsquedas que se realizaron sobre el tema se encontró que actualmente el anti-aliasing es un tema de relevancia y más en el campo de los videojuegos, debido a los agradables efectos que genera en los gráficos y cómo logra mejorar la calidad estética de lo que diariamente observamos a través de las pantallas. Pensando en futuro, se sugiere trabajar sobre las ideas que actualmente ya se han desarrollado, en especial sobre los algoritmos basados en MSAA, y observando sus costos computacionales con respecto a su antecesor el SSAA.\n"},{"id":9,"href":"/showcase/docs/shortcodes/Taller-2/espacios/","title":"Espacios","section":"Taller 2","content":" Espacios # Planteamiento del problema # En computación, la interacción 3D es una forma de interacción hombre-máquina en la que los usuarios pueden moverse y realizar interacciones en el espacio 3D. Tanto el ser humano como la máquina procesan información en la que la posición física de los elementos en el espacio 3D es relevante. El espacio 3D utilizado para la interacción puede ser el espacio físico real, una representación del espacio virtual simulado en el ordenador o una combinación de ambos. Cuando se utiliza el espacio físico real para la entrada de datos, el humano interactúa con la máquina realizando acciones mediante un dispositivo de entrada que detecta la posición 3D de la interacción humana, entre otras cosas. Cuando se utiliza para la salida de datos, la escena virtual 3D simulada se proyecta en el entorno real a través de un dispositivo de salida. Los principios de la interacción 3D se aplican en diversos ámbitos como el turismo, el arte, los juegos, la simulación, la educación, la visualización de información o la visualización científica.\nAntecedentes # La investigación sobre la interacción y la visualización en 3D comenzó en la década de los sesenta, de la mano de investigadores como Ivan Sutherland, Fred Brooks, Bob Sproull, Andrew Ortony y Richard Feldman. Pero no fue hasta 1962 cuando Morton Heilig inventó el simulador Sensorama. Proporcionaba retroalimentación de vídeo en 3D, así como movimiento, audio y retroalimentación para producir un entorno virtual. La siguiente etapa de desarrollo fue la finalización del trabajo pionero del Dr. Ivan Sutherland en 1968, la Espada de Damocles. Creó una pantalla montada en la cabeza que producía un entorno virtual en 3D presentando una imagen fija a la izquierda y a la derecha de ese entorno.\nLa disponibilidad de la tecnología, así como los costes poco prácticos, frenaron el desarrollo y la aplicación de los entornos virtuales hasta la década de 1980. Las aplicaciones se limitaron a empresas militares en Estados Unidos. Desde entonces, la investigación y los avances tecnológicos han permitido abrir nuevas puertas a la aplicación en otros ámbitos como la educación, el entretenimiento y la fabricación.\nHardware de entrada de la interfaz de usuario 3D: Estos dispositivos de hardware se denominan dispositivos de entrada y su objetivo es capturar e interpretar las acciones realizadas por el usuario. Los grados de libertad (DOF) son una de las principales características de estos sistemas. Los componentes clásicos de la interfaz (como el ratón y los teclados, y podría decirse que la pantalla táctil) suelen ser inadecuados para las necesidades de interacción no 2D. Estos sistemas también se diferencian según el grado de interacción física necesario para utilizar el dispositivo: los puramente activos necesitan ser manipulados para producir información, los puramente pasivos no lo necesitan. Las principales categorías de estos dispositivos son: dispositivos de entrada estándar (de escritorio), dispositivos de seguimiento, dispositivos de control, equipos de navegación, interfaces gestuales, ratones 3D e interfaces cerebro-ordenador.\nDispositivos de entrada de escritorio Dispositivos de seguimiento Nintendo Wii Remote (\u0026ldquo;Wiimote\u0026rdquo;) Dispositivos Google Tango Microsoft Kinect Movimiento de salto ¿Qué es una transformación del mundo? Una transformación de mundo cambia las coordenadas del espacio del modelo, donde los vértices se definen en relación con el origen local de un modelo, al espacio del mundo, donde los vértices se definen en relación con un origen común a todos los objetos de una escena. En esencia, la transformación del mundo coloca un modelo en el mundo; de ahí su nombre. La transformación del mundo puede incluir cualquier combinación de traslaciones, rotaciones y escalas.\nEl diagrama muestra la relación entre el sistema de coordenadas mundial y el sistema de coordenadas local de un modelo.\nCódigo (solución) y resultados # Código en p5.js function initPressure() { Pressure.set(\u0026#39;#uiCanvas\u0026#39;, { end: function(){ pressure = 0; }, change: function(force, event) { pressure = force; } }); Pressure.config({ polyfill: true, polyfillSpeedUp: 1000, polyfillSpeedDown: 300, preventSelect: true, only: null }); } Instrucciones para el manejo:\nPara empezar y detener la grabación del dibujo presione la letra \u0026ldquo;r\u0026rdquo;. Para borrar el dibujo presione la letra \u0026ldquo;c\u0026rdquo;. Para agrandar el tamaño de la brocha presione la letra \u0026ldquo;a\u0026rdquo;. Para disminuir el tamaño de la brocha presione la letra \u0026ldquo;m\u0026rdquo;. Para la realización del ejercicio se tomó como base la librería pressure.js de Javascript, con la cual fue posible realizar una medición de la presión ejercida y de esa manera evaluar este parámetro como la profundidad dentro del lienzo, así mismo se realizó la implementación de nuevas brochas añadidas desde las figuras 3D base que ofrece p5.\nEjemplos: Conclusiones y trabajo futuro # La exploración que se hizo con el 3D brush presentado durante las clases de Computación Visual, complementa las ideas y las lleva a la práctica, especialmente la del uso de las transformaciones de screen to world, cómo entendemos y manejamos estas interacciones, además de proponer nuevos estilos distintos a los tradicionalmente conocidos como ratón y teclado, que permiten explorar nuevas alternativas para el desarrollo de la computación visual y lo que puede ser un nuevo espacio para nuevas posibilidades desde escritorios con ambientes 3D hasta en general una interacción mucho más completa en todo aspecto con nuestras máquinas de uso diario.\nBasados en el desarrollo del taller, encontramos interés en el desarrollo de modelos 3D y software especializado que permita trabajar con un mayor grado de libertad. Consideramos que esta es una aproximación a lo que puede ser nuevas herramientas y áreas de trabajo en las cuales se permita manipular entornos inmersivos para la construcción de prototipos y modelos de la realidad que permitan avanzar en áreas tanto científicas como del entretenimiento, ya sean en animación de series, videojuegos, entre otros. Dado que este tipo de herramientas ofrecen la capacidad al usuario de crear mundos virtuales que puedan usarse con múltiples fines. Para trabajos futuros creemos que es importante evaluar la comodidad con la cual se manejan los controles, realizar estudios sobre su “ergonomía” e intuición para el usuario y experimentar si dicha percepción aumenta o disminuye con el uso continuo de la herramienta. Adicionalmente se promueve la idea de añadir más capacidades al entorno, como podría ser agregar la capacidad de añadir puntos de luces adicionales, permitir subir modelos ya prefabricados, guardar modelos hechos en la herramienta o que exista la posibilidad de crear su propia brocha.\n"},{"id":10,"href":"/showcase/docs/shortcodes/Taller-3/","title":"Taller 3","section":"Shortcodes","content":" Taller 3 # Esto fue el trabajo realizado para la tercera entrega de computación visual.\n"},{"id":11,"href":"/showcase/docs/shortcodes/Taller-3/masking/","title":"Masking","section":"Taller 3","content":" Masking # Planteamiento del problema # En el procesamiento de imágenes, un kernel, matriz de convolución o máscara es una pequeña matriz que se utiliza para desenfocar, afinar, dar relieve, detectar bordes, etc. Esto se consigue haciendo una convolución entre el núcleo y una imagen. La expresión general de una convolución es\nDonde g(x,y) es la imagen filtrada, f(x,y) es la imagen original, w es el kernel del filtro. Dependiendo de los valores de los elementos, un kernel puede causar una amplia gama de efectos.\nIdentidad. En matemáticas, una función de identidad, también llamada relación de identidad, mapa de identidad o transformación de identidad, es una función que siempre devuelve el valor que se utilizó como argumento, sin cambios. Es decir, cuando f es la función identidad, la igualdad f(X) = X es verdadera para todos los valores de X a los que se puede aplicar f. En este caso el kernel usado es el siguiente:\nComo resultado se obtendrá nuevamente la imagen original, como la descripción matemática lo indica, ya que no se hace ninguna modificación a la matriz y se deja el elemento original sin modificación alguna en la misma posición.\nDetección de crestas. En el procesamiento de imágenes, la detección de crestas es el intento de localizar crestas en una imagen, definidas como curvas cuyos puntos son máximos locales de la función, similares a las crestas geográficas. En este caso el kernel usado es el siguiente:\nComo resultado se obtiene una imagen cuyos bordes de los objetos se encuentran resaltados, permitiendo identificar claramente los cambios de color y las regiones donde están los distintos objetos.\nAfilado. Se usa un concepto similar al usado con la detección de bordes, con la idea de obtener una imagen con sus bordes delimitados de manera más explícita. En este caso el kernel usado es el siguiente:\nObteniendo una imagen dentro de la cual los bordes o límites de los objetos contenidos en ella parecen ser más delimitados y distinguibles que de la imagen original, otorgando una sensación de mayor contraste.\nCaja borrosa. Es un filtro lineal de dominio espacial en el que cada píxel de la imagen resultante tiene un valor igual al valor medio de sus píxeles vecinos en la imagen de entrada. Es una forma de filtro de paso bajo (\u0026ldquo;borroso\u0026rdquo;). En este caso el kernel usado es el siguiente:\nDesenfoque Gaussiano. Es el resultado de desenfocar una imagen mediante una función gaussiana (llamada así por el matemático y científico Carl Friedrich Gauss), normalmente se utiliza para reducir el ruido de la imagen y reducir los detalles. En este caso el kernel usado es el siguiente:\nEn otro sentido, también se hace consideración de dos herramientas que son útiles para el desarrollo del ejercicio y que permiten ampliar las posibilidades sobre el mismo. Como primero se analizó lo concebido como, una región de interés (a menudo abreviada ROI), que son muestras dentro de un conjunto de datos identificados para un propósito particular. Y luego, se incluyeron las ideas de una herramienta de magnificación/zoom, buscando imitar el efecto generado por las lupas. Para el zoom se utilizó la siguiente idea:\nEl punto rojo representa la posición del mouse, el círculo exterior representa el espacio sobre el cúal se va a mostrar el zoom y el círculo interior el espacio al cual se le va a hacer zoom. Lo que se plantea es cómo se ve en la imágen traer a cada uno de los puntos del círculo exterior el color correspondiente a la textura del círculo interior, esto se logra asumiendo que ambos círculos tienen una línea que conecta desde el centro de estos hasta sus respectivos bordes por lo que lo necesario para traer su correspondiente se logra mapeando el valor inicial que se encuentra en el rango [centro, radio círculo exterior] en el rango [centro, radio círculo interior], y con esto se consigue encontrar la textura correspondiente.\nAntecedentes # El concepto de ROI se ha utilizado habitualmente en muchos ámbitos de aplicación. Por ejemplo, en las imágenes médicas, los límites de un tumor pueden definirse en una imagen o en un volumen, con el fin de medir su tamaño. El límite endocárdico puede definirse en una imagen, quizás durante diferentes fases del ciclo cardíaco, por ejemplo, al final de la sístole y al final de la diástole, con el fin de evaluar la función cardíaca. En los sistemas de información geográfica (SIG), un ROI puede tomarse literalmente como una selección poligonal de un mapa 2D. En el reconocimiento óptico de caracteres, el ROI define los límites de un objeto considerado. Algunos ejemplos de los usos más comunes son:\nConjunto de datos 1D: un intervalo de tiempo o de frecuencia en una forma de onda, Conjunto de datos 2D: los límites de un objeto en una imagen, Conjunto de datos 3D: los contornos o superficies que delimitan un objeto (a veces conocido como Volumen de Interés (VOI)) en un volumen, Conjunto de datos 4D: el contorno de un objeto en o durante un intervalo de tiempo determinado en un volumen de tiempo. Código (solución) y resultados # Instrucciones de uso:\nSe tiene un selector donde se puede escoger cual es la máscara que se desea aplicar. El botón de chequeo de “Cámara” permite activar la cámara para que sea eso lo que se pasa al shader. El seleccionador de archivos permite subir una imágen o video para su procesamiento. El botón de chequeo “Región de interés” permite activar que la mascara solamente se aplique a la región circular que se quiera, la cual se determina por el mouse y el tamaño está ligado al slider que se encuentra debajo del selector de archivos. El botón de chequeo “Zoom” permite generar una región donde se magnifica la imágen, igualmente que la región de interés su posición se determina por el mouse y su tamaño por el slider. Código fragment shader precision mediump float; varying vec2 vTexCoord; uniform sampler2D tex0; uniform sampler2D vid0; uniform vec2 texOffset; uniform float mask[9]; uniform bool orig; uniform bool bord; uniform bool cam; uniform bool roi; uniform bool zoom; uniform float posY; uniform float posX; uniform float roiSize; float map2(float x, float in_min, float in_max, float out_min, float out_max) { return (x - in_min) * (out_max - out_min) / (in_max - in_min) + out_min; } void main() { vec4 convolution; vec4 texel; vec2 tc0 = vTexCoord + vec2(-texOffset.s, -texOffset.t); vec2 tc1 = vTexCoord + vec2( 0.0, -texOffset.t); vec2 tc2 = vTexCoord + vec2(+texOffset.s, -texOffset.t); vec2 tc3 = vTexCoord + vec2(-texOffset.s, 0.0); vec2 tc4 = vTexCoord + vec2( 0.0, 0.0); vec2 tc5 = vTexCoord + vec2(+texOffset.s, 0.0); vec2 tc6 = vTexCoord + vec2(-texOffset.s, +texOffset.t); vec2 tc7 = vTexCoord + vec2( 0.0, +texOffset.t); vec2 tc8 = vTexCoord + vec2(+texOffset.s, +texOffset.t); vec4 rgba[9]; if(!cam){ rgba[0] = texture2D(tex0, vec2(tc0.x,1.0-tc0.y)); rgba[1] = texture2D(tex0, vec2(tc1.x,1.0-tc1.y)); rgba[2] = texture2D(tex0, vec2(tc2.x,1.0-tc2.y)); rgba[3] = texture2D(tex0, vec2(tc3.x,1.0-tc3.y)); rgba[4] = texture2D(tex0, vec2(tc4.x,1.0-tc4.y)); rgba[5] = texture2D(tex0, vec2(tc5.x,1.0-tc5.y)); rgba[6] = texture2D(tex0, vec2(tc6.x,1.0-tc6.y)); rgba[7] = texture2D(tex0, vec2(tc7.x,1.0-tc7.y)); rgba[8] = texture2D(tex0, vec2(tc8.x,1.0-tc8.y)); texel = texture2D(tex0, vec2(vTexCoord.x,1.0-vTexCoord.y)); }else{ rgba[0] = texture2D(vid0, vec2(tc0.x,1.0-tc0.y)); rgba[1] = texture2D(vid0, vec2(tc1.x,1.0-tc1.y)); rgba[2] = texture2D(vid0, vec2(tc2.x,1.0-tc2.y)); rgba[3] = texture2D(vid0, vec2(tc3.x,1.0-tc3.y)); rgba[4] = texture2D(vid0, vec2(tc4.x,1.0-tc4.y)); rgba[5] = texture2D(vid0, vec2(tc5.x,1.0-tc5.y)); rgba[6] = texture2D(vid0, vec2(tc6.x,1.0-tc6.y)); rgba[7] = texture2D(vid0, vec2(tc7.x,1.0-tc7.y)); rgba[8] = texture2D(vid0, vec2(tc8.x,1.0-tc8.y)); texel = texture2D(vid0, 1.0 - vec2(vTexCoord.x,1.0-vTexCoord.y)); } for (int i = 0; i \u0026lt; 9; i++) { convolution += rgba[i]*mask[i]; } float pct = 0.0; pct = distance(vTexCoord,vec2(posX,1.0-posY)); if(roi){ if(pct\u0026lt;roiSize){ gl_FragColor = vec4(convolution.rgb, 1.0); }else{ gl_FragColor = vec4(texel.rgb, 1.0); } }else if(zoom){ if(pct\u0026lt;roiSize){ //vec2 coords2 = (vec2(map2(vTexCoord.x-posX, -roiSize/2.0, roiSize/2.0, -roiSize, roiSize),map2(vTexCoord.y-(1.0-posY), -roiSize/2.0, roiSize/2.0, -roiSize, roiSize))); vec4 xd = texture2D(tex0, vec2(vTexCoord.x + ((vTexCoord.x - posX) / 3.0), 1.0 - vTexCoord.y + (vTexCoord.y - 1.0 + posY) / 3.0)); gl_FragColor = vec4(xd.rgb ,1.0); }else{ gl_FragColor = vec4(texel.rgb, 1.0); } }else{ gl_FragColor = vec4(convolution.rgb, 1.0); } } Conclusiones y trabajo futuro # Para el desarrollo de esta actividad se encontró gran utilidad en la combinación de las regiones de interés junto con las máscaras, pudimos observar cómo es que dicha combinación permite al usuario estrictamente observar las partes que son relevantes para él sin alterar la imagen totalmente, nuevamente y como se encontró en la investigación, sus usos en el análisis de tests médicos y resultados de laboratorio puede ser prometedor, facilitando la labor de identificación de anomalías, así como también creemos que sobre el análisis no sólo en materia de medicina si no en general de fotografías y/o videos que requieran identificación de anomalías puede tener una amplia aplicación desarrollando las máscaras requeridas para cada caso.\nPara desarrollos futuros alentamos a desarrollar máscaras para casos específicos, como por ejemplo para la detección sobre imágenes satelitales de zonas deforestadas a través de los cambios en la vegetación y en su densidad. También pueden realizarse nuevas implementaciones sobre cómo se genera el zoom sobre la imágen como el conocido “efecto burbuja”, esto originado a partir de problemas que se observaron a la hora de intentar distintas implementaciones del efecto, generando distorsiones no deseadas en algunas áreas de la imagen proyectada, por lo que se sugiere estudiar el funcionamiento y las posibles causas.\n"},{"id":12,"href":"/showcase/docs/shortcodes/Taller-3/normal-mapping/","title":"Normal Mapping","section":"Taller 3","content":" Normal mapping # Planteamiento del problema # En los gráficos por ordenador en 3D, el mapeo normal, o mapeo de baches Dot3, es una técnica de mapeo de texturas que se utiliza para fingir la iluminación de baches y abolladuras, una implementación del mapeo de baches. Se utiliza para añadir detalles sin utilizar más polígonos. Un uso común de esta técnica es mejorar en gran medida la apariencia y los detalles de un modelo de pocos polígonos generando un mapa de normales a partir de un modelo de muchos polígonos o un mapa de altura.\nLos mapas de normales suelen almacenarse como imágenes RGB regulares en las que los componentes RGB corresponden a las coordenadas X, Y y Z, respectivamente, de la normal de la superficie.\nAntecedentes # En 1978 Jim Blinn describió cómo las normales de una superficie podían ser perturbadas para hacer que las caras geométricamente planas tuvieran una apariencia detallada La idea de tomar detalles geométricos de un modelo de alta poligonización fue introducida en \u0026ldquo;Fitting Smooth Surfaces to Dense Polygon Meshes\u0026rdquo; por Krishnamurthy y Levoy, Proc. SIGGRAPH 1996, donde este enfoque fue utilizado para crear mapas de desplazamiento sobre nurbs. En 1998, se presentaron dos artículos con ideas clave para transferir detalles con mapas normales de mallas de alta a baja poligonación: \u0026ldquo;Appearance Preserving Simplification\u0026rdquo;, de Cohen et al. SIGGRAPH 1998, y \u0026ldquo;A general method for preserving attribute values on simplified meshes\u0026rdquo;, de Cignoni et al. IEEE Visualization \u0026lsquo;98. El primero introdujo la idea de almacenar las normales de la superficie directamente en una textura, en lugar de los desplazamientos, aunque requería que el modelo de bajo detalle fuera generado por un algoritmo particular de simplificación restringida. Este último presentó un enfoque más sencillo que desacopla la malla poligonal alta y baja y permite la recreación de cualquier atributo del modelo de alto detalle (color, coordenadas de textura, desplazamientos, etc.) de una manera que no depende de cómo se creó el modelo de bajo detalle. La combinación de almacenar las normales en una textura, con el proceso de creación más general, sigue siendo utilizada por la mayoría de las herramientas disponibles actualmente.\nCódigo (solución) y resultados # Instrucciones de uso: En el seleccionador se puede intercambiar entre las 5 texturas predeterminadas que se tienen.\nCódigo vertex shader attribute vec3 aPosition; attribute vec2 aTexCoord; attribute vec3 aNormal; uniform mat4 uProjectionMatrix; uniform mat4 uModelViewMatrix; uniform sampler2D uNoiseTexture; varying vec2 vTexCoord; varying vec3 vNoise; void main() { vec4 noise = texture2D(uNoiseTexture, aTexCoord); vNoise = noise.rgb; vec4 positionVec4 = vec4(aPosition, 1.0); float amplitude = 1.0; positionVec4.xyz += (noise.rgb)/5.0 * aNormal * amplitude; gl_Position = uProjectionMatrix * uModelViewMatrix * positionVec4; vTexCoord = aTexCoord; } En esta implementación se codifica las normales en el espacio del objeto, de modo que los componentes rojo, verde y azul (rgb) se corresponden directamente con las coordenadas X, Y y Z.\nConclusiones y trabajo futuro # El trabajo realizado sobre el estudio tanto del bump mapping como del normal mapping permitió comprender los usos que una imagen sin aparente profundidad y solo con colores en ella puede tener si se usan adecuadamente los valores que dichos colores pueden representar, de tal manera que se altere la figura a la cual se le aplique dicha imagen, generando efectos visuales de texturas que imitan de mejor manera la realidad observada. También se encontró que dicha actividad puede ser realizada basados en modelos de iluminación, o con el uso de un geometry shader, pero debido a p5 no fue posible debido a que dentro de este no se encuentra la posibilidad de hacer uso de uno.\n"},{"id":13,"href":"/showcase/docs/shortcodes/Taller-3/texturing/","title":"Texturing","section":"Taller 3","content":" Texturing # Planteamiento del problema # LUMA: El luma representa el brillo de una imagen (la parte \u0026ldquo;en blanco y negro\u0026rdquo; o acromática de la imagen). El luma suele ir emparejado con la crominancia. El luma representa la imagen acromática, mientras que los componentes del croma representan la información del color. Es un término comúnmente utilizado en el procesamiento digital de imágenes para caracterizar a cada píxel. En los formatos digitales que siguen el estándar CCIR_601, la luma Y de un píxel se calcula con la fórmula Y = 0,299R + 0,587G + 0,114B.\nInverso del color. Una forma de pensar en la inversión del color es utilizar un modelo de color RGB. Se puede decir que todos los colores pueden representarse como una combinación de niveles de tres colores primarios diferentes: rojo, azul y verde. Por ejemplo, el azul puro sería 0% rojo, 0% verde y 100% azul. Si se resta cada uno de ellos de 100 para invertir sus valores, se obtiene un 100% de rojo, un 100% de verde y un 0% de azul. El rojo + el verde resultantes dan el amarillo, que es el color complementario del azul. El negro es 0% rojo, 0% verde y 0% azul, por lo que el inverso sería 100% rojo, 100% verde y 100% azul, que es el blanco. El gris podría representarse como un 50% de rojo, un 50% de verde y un 50% de azul (o algo parecido), por lo que el inverso del gris sería el gris. Por lo que basados en el modelo RGB obtenemos que el inverso del color es su color complementario, que puede ser encontrado al restar en una escala de 0 a 100, 100 a cada uno de sus valores correspondientes, rojo, verde y azul.\nTeñido. En este caso lo que se busca es teñir o tinturar una textura a partir de un color, para esto lo que se hace es multiplicar cada una de las componentes rgb del pixel de la textura con su correspondiente componente rgb del color de teñido, se realiza una multiplicación pues experimentalmente se halló que esta da un mejor realce al color de teñido, y permite que los valores se mantengan dentro del rango requerido, debido a que la multiplicación no supera los límites.\nEliminación. Dentro de la eliminación de un color, la idea fundamental es sustraer de la imagen y de cada pixel un porcentaje del color a eliminar, así por ejemplo, si lo que se desea es sustraer de una imagen las tonalidades azul, basándonos en un modelo de color RGB, lo adecuado es sustraerle a cada punto de color 0 en el espectro del rojo, 0 en el del verde y finalmente 100 en el azul, de tal manera que se elimina cualquier tonalidad de azul presente en la pintura y la imagen quedará ilustrada en términos de los colores restantes, rojo y verde.\nTeñido con interpolado. En este caso lo que se hizo fue generar una textura base con la cual se iba a teñir la textura objetivo, esta textura base se creó por medio de un cuadrado en el cual cada una de sus puntas tenía un color diferente y se generaba un degradado entre cada uno de los colores para llegar a los otros, luego de la misma manera que en el caso del primer teñido se multiplicó los componentes rgb del teñido objetivo por su correspondiente componente rgb pero esta vez no de un solo color de teñido sino del rgb del píxel correspondiente en la textura base.\nFoco de luz. El efecto de foco de luz se realizó mediante una técnica simillar a la de teñido con interpolado y a la de región de interés, por lo que se hace un teñido con tonos blancos, que es más fuerte conforme más cerca estén los pixeles del sitio donde se encuentren del mouse, y conforme esta distancia va aumentando estos tonos se van oscureciendo en una escala de grises hasta llegar al negro.\nCódigo (solución) y resultados # Instrucciones de uso:\nSe tiene un selector donde se puede escoger cual es la textura que se desea aplicar. El botón de chequeo de “Cámara” permite activar la cámara para que sea eso lo que se pasa al shader. El seleccionador de archivos permite subir una imágen o video para su procesamiento. El seleccionador de color permite enviar al shader el color para ciertas texturas, como teñido y eliminación. El botón “Randomize” permite generar valores aleatorios para las cuatro esquinas cuando se está en la textura teñida 2. Código fragment shader precision mediump float; varying vec4 color4; varying vec2 vTexCoord; uniform sampler2D tex0; uniform sampler2D vid0; uniform bool inv; uniform bool baw; uniform bool cam; uniform bool ten; uniform bool elm; uniform bool luz; uniform bool hsl; uniform vec2 mousePos; uniform vec3 colT; uniform float opc; float luma(vec3 texel) { return 0.299 * texel.r + 0.587 * texel.g + 0.114 * texel.b; } vec3 hslCol(vec3 texel){ float Cmin = min(min(texel.r,texel.g),texel.b); float Cmax = max(max(texel.r,texel.g),texel.b); float delta = Cmax - Cmin; float H = 0.0 ,S = 0.0 ,L = 0.0 ; if(delta == 0.0){ H = 0.0; S = 0.0; }else if(texel.r == Cmax){ H = 60.0 * (mod(((texel.g-texel.b)/delta), 6.0)); }else if(texel.g==Cmax){ H = 60.0 * (((texel.b-texel.r)/delta) + 2.0); }else{ H = 60.0 * (((texel.r-texel.g)/delta) + 4.0); } L = (Cmax + Cmin)/2.0; if(delta!=0.0){ S = delta/(1.0-(2.0*L - 1.0 \u0026gt; 0.0 ? 2.0*L-1.0 : -(2.0*L-1.0))); } return vec3(H,S,L); } void main() { vec2 uv = vTexCoord; uv = vec2(uv.x,1.0-uv.y); vec4 tex; if(!cam){ tex = texture2D(tex0, uv); }else{ tex = texture2D(vid0, uv); } float pct = 0.0; pct = distance(vTexCoord,vec2(mousePos.x,1.0-mousePos.y)); if(baw){ tex = vec4((vec3(luma(tex.rgb))), opc); }else if(inv){ tex = vec4(vec3(1.0) - tex.rgb, opc); }else if(ten){ tex = vec4((tex.rgb*colT.rgb), opc); }else if(elm){ tex = vec4(tex.rgb-colT.rgb, opc); }else if(hsl){ tex = vec4(tex.rgb*color4.rgb, opc); }else if(luz){ tex = vec4((tex.rgb+(0.5-vec3(pct))), 1.0); }else{ tex = vec4(tex.rgb, opc); } gl_FragColor = tex; } Conclusiones y trabajo futuro # Durante el desarrollo del ejercicio se estudiaron distintas manipulaciones de los colores de la imagen, pudiendo observar interesantes resultados que creemos son de utilidad para la industria de las artes gráficas y audiovisuales, dichos efectos explorados pueden ser usados para la evocación de sensaciones en los espectadores, adicionalmente de la capacidad que tiene para transformar la imagen percibida. Como trabajo futuro recomendamos estudiar nuevas formas o combinaciones de colores, esperando que tal vez combinaciones más complejas tanto en el campo de la iluminación como en el de los colores puedan ofrecer nuevas alternativas a las modificaciones que se pueden lograr. Así mismo durante el desarrollo pudimos evidenciar que para la técnica de teñido la multiplicación de los valores genera un resultado más natural para el ojo humano que una suma promedio entre los dos valores.\nAdicionalmente, se motiva al desarrollo de una interfaz que permita una manipulación por áreas de la imagen, procurando permitir que el usuario delimite qué espacios de quieren que se vean afectados por el efecto otorgando así mayor libertad a la hora de usar el software.\n"}]